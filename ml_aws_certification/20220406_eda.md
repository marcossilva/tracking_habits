## Types of Data
* Numerical
* Categorical
* Ordinal

## Data Distribution
* Normal - Probability Density Function (PDF) for continuous data; Probability Mass Function (PMF) for categorical data (as histograms)
* Poisson Distribution = Discrete Data
* Binomial Distribution = Discrete Data
* Bernoulli Distribution = Special Case of Binomial. Consist of multiple cases of Binomial

## Time Series Analysis
* Trends
* Seasonality
* Noise - Might add in your time series or be multiplicative

## Athena
* Serveless
* Works with Glue

## Quicksight
* Data Visualization Serverless Tool
* Data Sources: Redshift, Athena, S3, Aurora/RDS
* SPICE - Super-fast, Parallel, In-memory Calculation Engine
* Uses columnar storage, in-memory, machine code generation
* Accelerate interactive queries on large datasets
* Has 3 built-in Insights:
  * Anomaly Detection (random cut forest)
  * Forecasting
  * Auto-narratives (turns data into text)
* Graphs:
  * Bar Charts: Comparison, Distribution
  * Line/Area Charts: Changes over Time
  * Scatter Plots and Heat Maps: Correlation
  * Pie Charts: Aggregation
  * Tree Maps: Heirarchical Aggregation
  * Pivot Tables: Tabular Data
* HDFS or EMRFS (using S3 as backend)
* RDD (Spark)

## Spark MLLib

* Classification: logistic regression, naive Bayes * Regression
* Decision trees
* Recommendation engine (ALS)
* Clustering (K-Means)
* LDA (topic modeling)
* ML workflow utilities (pipelines, feature transformation, persistence)
* SVD, PCA, statistics

* Zeppelin + Spark
  * Can run Spark code interactively (like you can in the Spark shell) 
  * This speeds up your development cycle 
  * And allows easy experimentation and exploration of your big data 
  * Can execute SQL queries directly against SparkSQL 
  * Query results may be visualized in charts and graphs
  * Make Spark feel more like a data science tool!

## EMR: Choosing Instance Types

* Master node: 
  * m4.large if < 50 nodes, m4.xlarge if > 50 nodes
* Core & task nodes: 
  * m4.large is usually good 
  * If cluster waits a lot on external dependencies (i.e. a web crawler), t2. medium 
  * Improved performance: m4.xlarge 
  * Computation-intensive applications: high CPU instances 
  * Database, memory-caching applications: high memory instances 
  * Network / CPU-intensive (NLP. ML) — cluster computer instances

* Spot instances 
  * Good choice for task nodes
  * Only use on core & master if you're testing or very cost-sensitive; you're risking partial data loss

## What is feature engineering?

* Applying your knowledge of the data — and the model you're using - to create better features to train your model with. 
* Which features should I use? 
* Do I need to transform these features in some way? 
* How do I handle missing data? 
* Should I create new features from the existing ones?
* You can't just throw in raw data and expect good results 
* This is the art of machine learning; where expertise is applied
* “Applied machine learning is basically feature engineering” — Andrew Ng

## The Curse of Dimensionality

* Too many features can be a problem — leads to Sparse data
* Every feature is a new dimension
* Much of feature engineering is selecting aes most relevant to the problem at han
* This often is where domain knowledge comes into play
* Unsupervised dimensionality reduction techniques can also be employed to distill many features into fewer features 
  * PCA 
  * K-Means

## Imputing Missing Data: Mean Replacement (Bad)

* Replace missing values with the mean value from the rest of the column (columns, not rows! A column represents a single feature; it only makes sense to take the mean from other samples of the same feature.)
* Fast & easy, won't affect mean or sample size of overall data set
* Median may be a better choice than mean when **outliers** are present
* But it's generally pretty terrible.
* Only works on column level, misses correlations between features
  * Can't use on categorical features (imputing with most frequent value can work in this case, though)
  * Not very accurate

## Imputing Missing Data: Dropping (Bad)
* If not many rows contain missing data...
  * ...and dropping those rows doesn't bias your data...
  * ...and you don't have a lot of time... 
  * ...maybe it’s a reasonable thing to do.
* But, it’s never going to be the right answer for the “best” approach.
* Almost anything is better. Can you substitute another similar field perhaps? (i.e., review Summary vs. full text)

## Imputing Missing Data: Machine Learning (Best)

* KNN: Find K “nearest” (most similar) rows and average their values 
  * Assumes numerical data, not categorical 
  * There are ways to handle categorical data (Hamming distance), but categorical data is probably better served by...

* Deep Learning 
  * Build a machine learning model to impute data for your machine learning mode!! 
  * Works well for categorical data. Really well. But it's complicated.

* Regression 
  * Find linear or non-linear relationships between the missing feature and other features 
  * Most advanced technique: MICE (Multiple Imputation by Chained Equations)

## Imputing Missing Data: Just Get More Data

* What's better than imputing data? Getting more real data! 
* Sometimes you just have to try harder or collect more data

## What is unbalanced data?
* Large discrepancy between “positive” and “negative” cases 
  * i.e., fraud detection. Fraud is rare, and most rows will be not- fraud 
  * Don't let the terminology confuse you; “positive” doesn't mean “good"
    * It means the thing you're testing for is what happened.
    * If your machine earning model is made to detect fraud, then fraud is the positive case
* Mainly a problem with neural networks

## Oversampling
* Duplicate samples from the minority class
* Can be done at random

## Undersampling
* Instead of creating more positive Samples, remove negative ones
* Throwing data away is usually not the right answer
* Unless you are specifically trying to avoid “big data” scaling issues

## SMOTE
* Synthetic Minority Over-sampling TEchnique 
* Artificially generate new samples of the minority class using nearest neighbors 
  * Run K-nearest-neighbors of each sample of the minority class 
  * Create a new sample from the KNN result (mean of the neighbors)
* Both generates new samples and undersamples majority class 
* Generally better than just oversampling

## Adjusting thresholds
* When making predictions about a classification (fraud / not fraud), you have some sort of threshold of probability at which point you'll flag something as the positive case (fraud)
* If you have too many false positives, one way to fix that is to simply increase that threshold. 
  * Guaranteed to reduce false positives 
  * But, could result in more false negatives

## Dealing with Outliers

* Sometimes it's appropriate to remove outliers from your training data
* Do this responsibly! Understand why you are doing iS.
* For example: in collaborative meen a single user who rates thousands of movies cou d havea big ret on everyone else's ratings. That may not be esirable.
* Another example: in web log data, outliers may represent bots or other agents that should be iscarded.
* But if someone really wants the mean income of US citizens for example, don’t toss out billionaires just because you want to.
* Remember AWS's Random Cut Forest algorithm creeps into many of its services — it is made for outlier detection 
  * Found within QuickSight, Kinesis Analytics, SageMaker, and more

## Binning

* Bucket observations together based on ranges of values.
* Example: estimated ages of people 
  * Put all 20-somethings in one classification, 30-somethings in another, etc. 
* Quantile binning categorizes data by their place in the data distribution 
  * Ensures even sizes of bins
* Transform numeric data to ordinal ata
* Especially useful when there is uncertainty in the measurements

## Transtorming

* Applying some function to a feature to make it better suited for training 
* Feature data with an exponential trend may benefit from a logarithmic transform 
* Example: YouTube recommendations 
  * A numeric feature x is also represented by x sqrt(x) 
  * This allows learning of super and sub-linear functions

(https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)

## Encoding

* Transforming data into some new representation required by the model
* One-hot encoding 
  * Create “buckets” for every category 
  * The bucket for your category has a 1, all others have a 0 
  * Very common in deep learning, where categories are represented by individual output “neurons”

## Scaling / Normalization

* Some models prefer feature data to be normally distributed around 0 (most neural nets)
* Most models require feature data to at least be scaled to comparable values 
  * Otherwise features with larger magnitudes will have more weight than they shoul 
  * Example: modeling age and income as features — incomes will be much higher values than ages 
* Scikit_learn has a preprocessor module that helps (MinMaxScaler, etc)
* Remember to scale your results back up

## Shuffling

* Many algorithms benefit from shuffling their training data
* Otherwise they may learn from residual signals in the training data resulting from the order in which they were collected

## Ground Truth?

* Sometimes you don't have training data at all, and it needs to be generated by humans first.
* Example: training an image classification model. Somebody needs to tag a bunch of images with what they are images of before training a neural network
* Ground Truth manages humans who will label your data for training purposes
* As this model learns, only images the model isn’t sure about are sent to human labelers
* This can reduce the cost of labeling jobs by 70%
* Rekognition 
* AWS service for image recognition 
* Automatically classify images
* Comprehend
* AWS service for text analysis and topic modeling 
* Automatically classify text by topics, sentiment
* Any pre-trained model or unsupervised technique that may be helpful

## TF-IDF
* We actually use the log of the IDF, since word frequencies are distributed exponentially. That gives us a better weighting of a words overall popularity
* TF-IDF assumes a document is just a “bag of words" 
  * Parsing documents into a bag of words can be most of the work 
  * Words can be represented as a hash value (number) for efficiency
* What about synonyms? Various tenses? Abbreviations? Capitalizations? Misspellings?
* Unigrams, bigrams, etc.