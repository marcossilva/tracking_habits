## Deep Learning

* Tensorflow / Keras 
* MXNet
* tanh over sigmoid because it squashes between [-1, 1] which leads to a mean of 0 which is preferrable
* ReLU, Leaky ReLU, PReLU, ELU, Swish, Maxout
* Softmax - useful for classification as where sigmoid is usefull for multilabeling
* Conv2D -> MaxPooling2D => Dropout -> Flatten -> Dense -> Dropout -> Softmax
* LeNet, AlexNet, GoogLeNet, ResNet
* RNN for time-series data or sequence of arbitrary length as tet os sound
* Sequence 2 Sequence, Sequence 2 Vector (word2vec), Vector 2 Sequence, Encoder Decoder
* RNNCell, LSTM, GRU
* P3: 8 Tesla V100 GPU's; P2: 16 K80 GPU's; G3: 4 M60 GPU's (all Nvidia chips)
* Learning rate. Too high might overshoot and too low might not converge.
*** Small batch sizes tend to not get stuck in local minima
* Large batch sizes can converge on the wrong solution at random 
* Large learning rates can overshoot the correct solution
* Small learning rates increase training time**
* optimizer; regularization; dropout; early stopping;

## Neural Network Regularization Techniques
* Preventing overfitting
  * Models that are good at making predictions on the data they were trained on, but not on new data it hasn't seen before
  * Overfitted models have learned patterns in the training data that don’t generalize to the real world
  * Often seen as high accuracy on training data set, but lower accuracy on test or evaluation data set.
  * When training and evaluating a model, we use training, evaluation, and testing data sets.
* Regularization techniques are intended to prevent overfitting
* Overfitting -> Use a simpler model (less neurons or layers)
* Dropout -> Randomly drop neurons from the network to prevent overfitting
* Early Stopping -> Stop training when the model stops improving after a certain number of epochs (usually called patience)

## The Vanishing Gradient Problem
* The vanishing gradient problem is a problem in neural networks where the gradient of the loss function is zero.
* Common in neural networks, it is a problem in which the model is unable to learn anything useful from the data.
* Occurs in RNNs
* LSTM and ResNet are design to deal with this problem
* Gradient Checking
  * A debugging technique
  * Numerically check the derivatives computed during training
  * Useful for validating code of neural network training 
  * But you're probably not going to be writing this code...

## L1 and L2
* L1: sum of weights 
  * Performs feature selection — entire features go to 0 
  * Computationally inefficient 
  * Sparse output

* L2: sum of square of weights 
  * All features remain considered, just weighted 
  * Computationally efficient 
  * Dense output

* Feature Selection with L1 Regularization can reduce dimensionality. L1 is more expensive to compute but could reduce greatly the number of parameters in the model.
* If all the features are important L2 is probably better

## Confusion Matrix
* A confusion matrix is a table that is often used to describe the performance of a classification model
* The diagonal elements represent the number of true positives (correctly classified examples) and the off-diagonal elements represent the number of false positives and negatives

## Recall
* TP / (TP + FN)
* % of negatives wrongly predicted
* AKA as Sensitivity, TPR, Completeness
* Good metric when you care a lot about false negatives (fraud detection)

|                     | Actual Fraud | Actual Not Fraud |
|---------------------|--------------|------------------|
| Predicted Fraud     | 5            | 20               |
| Predicted Not Fraud | 10           | 100              |

Recall = TP / (TP + FN) = 5 / (5+10) = 0.33

## Precision
* TP / (TP + FP)
* AKA Correct Positives
* % of relevant results
* Good metric when you care a lot about false positives (medical screening, drug testing)

Precision = TP / (TP + FP) = 5 / (5+20) = 0.2

## Other Metrics
* Specificity = TN / (TN + FP) = True Negative Rate
* F1 Score = 2 * Precision * Recall / (Precision + Recall)
* RMSE = Root Mean Squared Error

## ROC Curve
* Receiver Operating Characteristic (ROC) curve is a plot of the true positive rate (recall) vs the false positive rate
* Points above the diagonal represent good classifications (better than random)
* Ideal curve would just be a point at the top left corner
* The more the curve is bent toward the top left corner, the better the classifier

## AUC
* Area Under the ROC Curve (AUC)
* Probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one
* ROC AUC of 0.5 is a useless classifier, 1.0 is perfect
* Commonly used metric for comparing classifiers

## Ensemble Methods
* Take multiple models or variations of the same model and combine them to create a new model

## Bagging
* Generate N new training set by random sampling with replacement
* Each resampled model can be trained in **parallel**

## Boosting
* Observations are weighted
* Some will take part in new training sets more often
* Training is sequential; each classifier takes into account the previous one’s success.

## Bagging vs. Boosting
* XGBoost is the latest hotness
* Boosting generally yields better accuracy
* But bagging avoids overfitting
* Bagging is easier to parallelize

## SageMaker
* Deploy model, evaluate results in production
* Fetch, clean, and prepare data
* Train and evaluate a model

### Data Prep
* Data must come from S3
* * Ideal format varies with algorithm — often it is RecordlO / Protobuf
* Apache Spark integrates with SageMaker
* Scikit_learn, numpy, pandas all at your disposal within a notebook

### Training on SageMaker

* Create a training job 
  * URL of S3 bucket with training data 
  * ML compute resources 
  * URL of S3 bucket for output 
  * ECR path to training code 
* Training options 
  * Built-in training algorithms 
  * Spark MLLib 
  * Custom Python Tensorflow / MXNet code 
  * Your own Docker image
  * Algorithm purchased from AWS marketplace

### Deploying Trained Models

* Save your trained model to S3
* Can deploy two ways: 
  * Persistent endpoint for making individual predictions on demand 
  * SageMaker Batch Transform to get predictions for an entire dataset

* Lots of cool options
  * Inference Pipelines for more complex processing
  * SageMaker Neo for deploying to edge devices
  * Elastic Inference for accelerating deep learning models
  * Automatic scaling (increase # of endpoints as needed

## Linear Learner
* Linear regression
* Handle regression and classification with a linear threshold
* Input:
  * RecordlO-wrapped protobuf (Float32 data only!)
  * CSV (First column assumed to be the label)
  * File or Pipe mode both supported (Pipe mode instead of File mode if is taking too long to start)
* Preprocessing
  * Training data must be normalized (so all features are weighted the same)
  * Linear Learner can do this for you automatically 
  * Input data should be shuffled
* Training
  * Uses stochastic gradient descent
  * Choose an pueeasuan algorithm (Adam, AdaGrad, SGD, etc)
  * Multiple models are optimized in parallel 
  * Tune L1 (feature selection), L2 (smoothing) regularization
* Validation 
  * Train multiple models in parlelle and select the most optimal model is selected
* Hyperparameters:
  * Balance_multiclass_weights - Gives each class equal importance in loss functions
  * Learning_rate
  * mini_batch_size
  * L1 (regularization)
  * Wd (weight decay - L2 regularization)
* Training
  * Single or multi-machine CPU or GPU
  * Multi-GPU does not help

## XGBoost
* eXtreme Gradient Boosting 
  * Boosted group of decision trees
  * New trees made to correct the errors of previous trees
  * Uses gradient descent to minimize loss as new trees are added
* It's been winning a lot of Kaggle competitions and it's fast, too
* Can be used for classification or regression using regression trees
* Input:
  * XGBoost is weird, since it's not made for SageMaker. It's just open source XGBoost
  * So, it takes CSV or libsvm input only. 
  * Protobuf not accepted.
* Models are serialized/deserialized with Pickle 
* Can use as a framework within notebooks (Sagemaker.xgboost)
* Or as a built-in SageMaker algorithm
* Hyperparameters:
  * Subsample (Prevents overfitting) 
  * Eta (Step size shrinkage, prevents overfitting)
  * Gamma (Minimum loss reduction to create a partition; larger = more conservative)
  * Alpha (L1 regularization term; larger = more conservative)
  * Lambda (L2 regularization term; larger = more conservative)
* Training:
  * Uses CPU's only
  * Is memory-bound, not compute- bound
  * So, M4 is a good choice

## Seq2Seq
* Input is a sequence of tokens, output is a sequence of tokens
* Machine Translation 
* Text summarization 
* Speech to text
* Implemented with RNN's and CNN's with attention
* Input:
* RecordIO-Protobuf
  * Tokens must be integers (this is unusual, since most algorithms want floating point data.)
* Start with tokenized text files
* Convert to protobuf using sample code
  * Packs into integer tensors with vocabulary iles
* A lot like the TF/IDF lab we did earlier.
* Must provide training data, validation data, and vocabulary files. The vocabulary files are used to encode the input data.
* Training for machine translation can take days, even on SageMaker
* Pre-trained models are available
* See the example notebook
* Public training datasets are available for specific translation tasks
* Hyperparameters:
  * Batch_size
  * Optimizer_type (adam, sgd, rmsprop) * Learning_rate
  * Num_layers_encoder
  * Num_layers_decoder
* Can optimize on: 
  * Accuracy (Vs. provided validation dataset)
  * BLEU score (Compares against multiple reference translations)
  * Perplexity (Cross-entropy)
* Training:
  * Can only use GPU instance types (P3 for example)
  * Can only use a single machine for training
  * But can use multi-GPU's on one machine

## DeepAR
* Forecasting one-dimensional time series data
* Uses RNN’s
* Allows you to train the same model over several related time series
* Finds frequencies and seasonality
* Input:
  * JSON lines format (Gzip or Parquet)
  * Each record must contain: 
    * Start: the starting time stamp 
    * Target: the time series values
  * Each record can contain:
    * Dynamic_feat: dynamic features (such as, was a promotion applied to a product in a time series product purchases)
    * Cat: categorical features
* Always include entire time series for training, testing, and inference
* Use entire dataset as test set, remove last time points for training. Evaluate on withheld values.
* Don't use very large values for prediction length (> 400)
* Train on many time series and not just one when possible
* Context_length
* Hyperparameters:
  * Number of time points the model sees before making a prediction
  * Can be smaller than seasonalities; the model will lag one year anyhow.
  * Epochs
  * mini_batch_size * Learning_rate
  * Num_cells
* Training:
  * Can use CPU or GPU * Single or multi machine
  * Start with CPU (C4.2xlarge, C4.4xlarge)
  * Move up to GPU if necessary * Only helps with larger models 
  * CPU-only for inference * May need larger instances for tuning

## BlazingText
* Text classification 
  * Predict labels for a sentence 
  * Useful in web searches, information retrieval
  * Supervised
* Word2vec
  * Creates a vector representation of words
  * Semantically similar words are represented by vectors Close to each other
  * This is called a word embedding
  * It is useful for NLP but is not an NLP algorithm in itself!
  * Used in machine translation, sentiment analysis
  * Remember it only works on individual words, not sentences or documents
* Input:
  * For supervised mode (text classification): 
    * One sentence per line 
    * First “word” in the sentence is the string __label__ followed by the label 
    * Also, “augmented manifest text format”
  * Word2vec just wants a text file with one training sentence per line.
* Word2vec has multiple modes
  * Cbow (Continuous Bag of Words) - Ignores order
  * Skip-gram
  * Batch skip-gram
    * Distributed computation over many CPU nodes
* Hyperparameters:
  * Word2vec: 
    * Mode (batch_skipgram, skipgram, cbow) 
    * Learning_rate 
    * Window_size 
    * Vector_dim 
    * Negative_samples
  * Text classification: 
    * Epochs 
    * Learning_rate 
    * Word_ngrams 
    * Vector_dim
* Training:
  * For cbow and skipgram, recommend a single ml.p3.2xlarge 
  * Any single CPU or single GPU instance will work 
  * For batch_skipgram, can use single or multiple CPU instances
  * For text classification, C5 recommended if less than 2GB training data. For larger data sets, use a single GPU instance (ml.p2.xlarge or ml.p3.2xlarge)

## Objetc2Vec
* Abitrary dimensionality-reduction embedding layer
* Compute NN of objects
* Visualize Clusters
* Unsupervised
* Input
  * Data must be tokenized into integers
  * Training data consists of pairs of tokens and/or sequences of tokens 
    * Sentence — sentence 
    * Labels-sequence (genre to description?) 
    * Customer-customer 
    * Product-product 
    * User-item
* Process data into JSON Lines and shuffle it
* Train with two input channels, two encoders, and a comparator
* Encoder choices:
  * Average-pooled embeddings
  * CNN's
  * Bidirectional LSTM
* Comparator is followed by a feed-forward neural network
* In summary it's a siamese network probably with triplets loss
* Hyperparameters
  * The usual deep learning ones...
    * Dropout, early stopping, epochs, learning rate, batch size, layers, activation function, optimizer, weight decay
  * Encl_network, enc2_network
    * Choose hcnn, bilstm, pooled_embedding
* Training:
* **Can only train on a single machine** (CPU or GPU, multi-GPU OK) 
  * Ml.m5.2xlarge 
  * Ml.p2.xlarge 
  * If needed, go up to ml.m5.4xlarge or ml.m5.12xlarge 
  * Inference: use ml.p2.2xlarge
* Use INFERENCE_PREFERRED_MODE environment variable to optimize for encoder embeddings rather than classification or regression.

## ObjectDetection
* Identify objects in image with bounding boxes
* Detects and classifies objects with a single deep neural network
* Classes are accompanied by confidence scores
* Can train from scratch, or use pre-trained models based on ImageNet
* Input
  * RecordlO or image format (jpg or png)
  * With image format, supply a JSON file for annotation data for each image
* Takes an image as input, outputs all instances of objects in the image with categories and confidence scores
* Uses a CNN with the Single Shot multibox Detector (SSD) algorithm
* The base CNN can be VGG-16 or ResNet-50 
* Transfer learning mode / incremental training
* Use a pre-trained model for the base network weights, instead of random initial weights
* Uses flip, rescale, and jitter internally to avoid overfitting
* Hyperparameters
  * Mini_batch_size
  * Learning_rate
  * Optimizer
    * Sgd, adam, rmsprop, adadelta
* Training
* Use GPU instances for training (multi-GPU and multi-machine OK)
* MI.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8clarge, ml.p3.16xlarge
* Use CPU or CPU for inference
  * C5, M5, P2, P3 all OK

## Image Classification
* Assign one or more labels to an image
* Doesn't tell you where objects are, just what objects are in the image
* Input
* Apache MXNet RecordlO 
* Not protobuf!
* This is for interoperability with other deep learning rameworks.
* Or, raw jpg or png images
* Image format requires .lst files to associate image indéx, class label, and path to the image
* Augmented Manifest Image Format enables Pipe mode
* ResNet CNN under the hood 
* Full training mode
  * Network initialized with random weights 
* Transfer learning mode
  * Initialized with pre-trained weights
  * The top fully-connected layer is initialized with random weights
  * Network is fine-tuned with new training data
* Default image size is 3-channel 224x224 (ImageNet's dataset)
* Hyperparameters
  * The usual suspects for deep learning
    * Batch size, learning rate, optimizer
  * Optimizer-specific parameters
    * Weight decay, beta 1, beta 2, eps, gamma
* Training
  * GPU instances for training (P2, P3) Multi-GPU and multi-machine OK.
  * CPU or GPU for inference (C4, P2, P3)

## Semantic Segmentation
* Pixel-level object classification
* Different from image classification — that assigns labels to whole images
* Different from object detection — that assigns labels to bounding boxes
* Useful for self-driving vehicles, medical imaging diagnostics, robot sensing
* Produces a segmentation mask
* Input
  * JPG Images and PNG annotations 
  * For both training and validation 
  * Label maps to describe annotations
  * Augmented manifest image format supported for Pipe mode.
  * JPG images accepted for inference
* Built on MXNet Gluon and Gluon CV
* Choice of 3 algorithms:
  * Fully-Convolutional Network (FCN)
  * Pyramid Scene Parsing (PSP)
  * DeepLabV3
* Choice of backbones:
  * ResNet50
  * ResNet101
  * Both trained on ImageNet
* Incremental training, or training from scratch, supported too
* Hyperparameters
  * Epochs, learning rate, batch size, optimizer, etc
  * Algorithm 
  * Backbone
* Training
  * **Only GPU supported for training** (P2 or P3) on a single machine only
  * Specifically ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge, ml.p3.8xlarge, or ml.p3.16xlarge
  * Inference on CPU (C5 or M5) or GPU (P2 or P3)

## Random Cut Forest
* Anomaly detection 
* Unsupervised
* Detect unexpected spikes in time series data
* Breaks in periodicity 
* Unclassifiable data points
* Assigns an anomaly score to each data point
* Based on an algorithm developed by Amazon that they seem to be very proud of!
* Input
  * RecordlO-protobuf or CSV 
  * Can use File or Pipe mode on either
  * Optional test channel for computing accuracy, precision, recall, and F1 on labeled data (anomaly or not)
* Creates a forest of trees where each tree is a partition of the training data; looks at expected change in complexity of the tree as a result of adding a point into it
* Data is sampled randomly
* Then trained
* RCF shows up in Kinesis Analytics as well; it can work on streaming data too.
* Hyperparameters
  * Num_trees 
    * Increasing reduces noise
  * Num_samples_per_tree
    * Should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous to normal data
* Training
  * Does not take advantage of GPUs
  * Use M4, C4, or C5 for training
  * ml.c5.xl for inference

## Neural Topic Model
* Organize documents into topics
* Classify or summarize documents based on topics
* It's not just TF/IDF
  * “bike”, “car”, “train”, “mileage”, and “speed” might classify a document as “transportation” for example (although it wouldn't know to call it that)
* Unsupervised
  * Algorithm is “Neural Variational Inference”
* Training
* Four data channels 
  * “train” is required
  * “validation”, “test”, and “auxiliary” optional 
* recordlO-protobuf or CSV
* Words must be tokenized into integers
  * Every document must contain a count for every word in the vocabulary in CSV
  * The “auxiliary” channel is for the vocabulary 
* File or pipe mode
* You define how many topics you want
* These topics are a latent representation based on top ranking words
* One of two topic modeling algorithms in SageMaker — you can try them both!
* Hyperparameters
  * Lowering mini_batch_size and learning_rate can reduce validation loss 
  * At expense of training time
  * Num_topics
* Training
  * GPU or CPU
  * GPU recommended for training
  * CPU OK for inference
  * CPU is cheaper

## LDA
* Latent Dirichlet Allocation
* Another topic modeling algorithm 
  * Not deep learning
* Unsupervised 
  * The topics themselves are unlabeled; they are just groupings of documents with a shared subset of words 
* Can be used for things other than words 
  * Cluster customers based on purchases 
  * Harmonic analysis in music
* Input:
  * Train channel, optional test channel 
  * recordlO-protobuf or CSV
  * Each document has counts for every word in vocabulary (in CSV format)
  * Pipe mode only supported with recordlO
* Unsupervised; generates however many topics you specify
* Optional test channel can be used for scoring results
* Per-word log likelihood
* Functionally similar to NTM, but CPU-based
* Therefore maybe cheaper / more efficient
* Hyperparameters
  * Num_topics
  * AlphaO
    * Initial guess for concentration parameter
    * Smaller values generate sparse topic mixtures
    * Larger values (>1.0) produce uniform mixtures
* Training
  * Single-instance CPU training

## KNN
* K-Nearest-Neighbors
* Simple classification or regression algorithm
* Classification 
  * Find the K closest points to a sample point and return the most frequent label 
* Regression
  * Find the K closest points to a sample point and return the average value
* Input
  * Train channel contains your data 
  * Test channel emits accuracy or MSE
  * recordlO-protobuf or CSV training 
  * First column is label
  * File or pipe mode on either
* Data is first sampled
* SageMaker includes a dimensionality reduction stage
  * Avoid sparse data (“curse of dimensionality”)
  * At cost of noise / accuracy
  * “sign” or “fjlt" methods
* Build an index for looking up neighbors
* Serialize the model
* Query the model for a given K
* Hyperparameters:
  * K
  * Sample_size
* Training
  * Training on CPU or GPU 
  * Ml.m5.2xlarge 
  * Ml.p2.xlarge
* Inference 
  * CPU for lower latency
  * GPU for higher throughput on large batches

## K-Means
* Unsupervised clustering
* Divide data into K groups, where members of a group are as similar as possible to each other 
* You define what “similar” means 
* Measured by Euclidean distance
* Web-scale K-Means clustering
* Input:
  * Train channel, optional test 
    * Train ShardedByS3Key, test FullyReplicated
  * recordlO-protobuf or CSV 
  * File or Pipe on either
* Every observation mapped to n-dimensional space (n = number of features)
* Works to optimize the center of K clusters 
  * “extra cluster centers” may be specified to improve accuracy (which end up getting reduced to k) 
  * K = k*x 
* Algorithm: * Determine initial cluster centers 
  * Random or k-means++ approach 
  * K-means++ tries to make initial clusters far apart
* Iterate over training data and calculate cluster centers
* Reduce clusters from K to k * Using Lloyd’s method with kmeans++
* Hyperparameters
  * K
    * Choosing K is tricky
    * Plot within-cluster sum of squares as function of K
    * Use “elbow method"
    * Basically optimize for tightness of clusters
  * Mini_batch_size
  * Extra_center_factor
  * Init_method
* Training
* CPU or GPU, but CPU recommended
* Only one GPU per instance used on GPU
* So use p*.xlarge if you're going to use GPU

## PCA
* Principal Component Analysis
* Dimensionality reduction 
  * Project higher-dimensional data (lots of features) into lower-dimensional (like a 2D plot) while minimizing loss of information 
  * The reduced dimensions are called components 
  * First component has largest possible variability 
  * Second component has the next largest...
* Unsupervised
* Input
  * recordlO-protobuf or CSV
  * File or Pipe on either
* Covariance matrix is created, then singular value decomposition (SVD)
* Two modes
  * Regular
    * For sparse data and moderate number of observations and features
    * Randomized 
      * For large number of observations and features 
      * Uses approximation algorithm
* Hyperparameters
  * Algorithm_mode
  * Subtract_mean 
    * Unbias data
* Training
  * GPU or CPU
  * It depends “on the specifics of the input data”

## Factorization Machines
* Dealing with sparse data
  * Click prediction
  * Item recommendations
  * Since an individual user doesn't interact with most pages / products the data is sparse
* Supervised
  * Classification or regression
* Limited to pair-wise interactions
  * User -> item for example
* Input
  * recordiO-protobuf with Float32
  * Sparse data means CSV isn't practical
* Finds factors we can use to predict a classification (click or not? Purchase or not?) or value (predicted rating?) given a matrix representing some pair of things (users & items?)
* Usually used in the context of recommender systems
* Hyperparameters
  * Initialization methods for bias, factors, and linear terms
  * Uniform, normal, or constant
  * Can tune properties of each method
* Training
  * CPU or GPU
  * CPU recommended
  * GPU only works with dense data

## IP Insights
* Unsupervised learning of IP address usage patterns
* Identifies suspicious behavior from IP addresses 
* Identify logins from anomalous IP's
* Identify accounts creating resources from anomalous IP's
* Input
  * User names, account ID's can be fed in directly; no need to pre-process
  * Training channel, optional validation (computes AUC score)
  * CSV only 
    * Entity, IP
* Uses a neural network to learn latent vector representations of entities and IP addresses.
* Entities are hashed and embedded
* Need sufficiently large hash size
* Automatically generates negative samples during training by randomly pairing entities and IP's
* Hyperparameters
  * Num_entity_vectors 
  * Hash size 
  * Set to twice the number of unique entity identifiers 
  * Vector_dim 
  * Size of embedding vectors 
  * Scales model size 
  * Too large results in overfitting
  * Epochs, learning rate, batch size, etc.
* Training
  * CPU or GPU 
  * GPU recommended 
  * Ml.p3.2xlarge or higher 
  * Can use multiple GPU's
  * Size of CPU instance depends on vector_dim and num_entity_vectors

## Reinforcement Learning
* You have some sort of agent that “explores” some space 
* As it goes, it learns the value of different state changes in different conditions
* Those values inform subsequent behavior of the agent
* Examples: 
  * Pac-Man, Cat & Mouse game (game Al) 
  * Supply chain management
  * HVAC systems
  * Industrial robotics
  * Dialog systems
  * Autonomous vehicles
* Yields fast on-line performance once the space has been explored
* Q-Learning
* You have: 
  * A set of environmental states s 
  * A set of possible actions in those states a 
  * A value of each state/action Q
* Start off with Q values of 0
* Explore the space
* As bad things happen after a given state/action, reduce its Q 
* As rewards happen after a given state/action, increase its Q
* The exploration problem
* How do we efficiently explore all of the possible states? 
  * Simple approach: always choose the action for a given state with the highest Q. If there's a tie, choose at random
    * But that's really inefficient, and you might miss a lot of paths that way
  * Better way: introduce an epsilon term 
    * If arandom number is less than epsilon, don't follow the highest Q, but choose at random
* That way, exploration never totally stops 
* Choosing epsilon can be tricky
* Markov Decision Process
  * From Wikipedia: Markov decision processes (MDPs) provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.
  * Sound familiar? MDP'’s are just a way to describe what we just did using mathematical notation.
  * States are still described as s and s' 
  * State transition functions are described as P,(s,s’) 
  * Our "Q" values are described as a reward function R,(s, s’)
  * Even fancier words! An MDP is a discrete time stochastic control Process.
* You can make an intelligent Pac-Man in a few steps: 
  * Have it semi-randomly explore different choices of movement (actions) given different conditions (states)
  * Keep track of the reward or penalty associated with each choice for a given state/action (Q)
  * Use those stored Q values to inform its future choices
* Pretty simple concept. But hey, now you can say you understand reinforcement learning, Q- learning, Markov Decision Processes, and Dynamic Programming!
* Uses a deep learning framework with Tensorflow and MXNet
* Supports Intel Coach and Ray RIlib toolkits.
* Custom, open-source, or commercial environments supported. 
  * MATLAB, Simulink 
  * EnergyPlus, RoboSchool, PyBullet 
  * Amazon Sumerian, AWS RoboMaker
* Can distribute training and/or environment rollout
* Multi-core and multi-instance
* Environment
  * The layout of the board / maze / etc
* State
  * Where the player / pieces are
* Action
  * Move in a given direction, etc
* Reward
  * Value associated with the action from that state
* Observation
  * i.e., surroundings in a maze, state of chess board
* Hyperparameters
  * Parameters of your choosing may be abstracted
  * Hyperparameter tuning in SageMaker can then optimize them
* Training
  * No specific guidance given in developer guide
  * But, it's deep learning — so GPU's are helpful
  * And we know it supports multiple instances and cores

## Automatic Model Tuning With SageMaker
* How do you know the best values of learning rate, batch size, depth, etc?
* Often you have to experiment
* Problem blows up quickly when you have many different hyperparameters; need to try every combination of every possible value somehow, train a model, and evaluate it every time
* Define the hyperparameters you care about and the ranges you want to try, and the metrics you are optimizing for
* SageMaker spins up a “HyperParameter Tuning Job” that trains as many combinations as you'll allow 
  * Training instances are spun up as needed, potentially a lot of them
* The set of hyperparameters producing the best results can then be deployed as a model
* MetaLearner
* Don't optimize too many hyperparameters at once 
* Limit your ranges to as small a range as possible 
* Use logarithmic scales when appropriate
* Don't run too many training jobs concurrently 
  * This limits how well the process can learn as it goes
* Make sure training jobs running on multiple instances report the correct objective metric in the end
* It learns as it goes, so it doesn't have to try every possible combination

## SageMaker and Spark
* Pre-process data as normal with Spark 
  * Generate DataFrames 
* Use sagemaker-spark library 
* SageMakerEstimator 
  * KMeans, PCA, XGBoost
* SageMakerModel
* Connect notebook to a remote EMR cluster running Spark (or use Zeppelin)
* Training dataframe should have: 
  * A features column that is a vector of Doubles 
  * An optional labels column of Doubles 
* Call fit on your SageMakerEstimator to get a SageMakerModel 
* Call transform on the SageMakerModel to make inferences
* Works with Spark Pipelines as well.
* Allows you to combine pre-processing big data in Spark with training and inference in SageMaker.

## New SageMaker Features
* SageMaker Studio. Visual IDE for ML
* SageMaker Notebooks. Create and share. Switch betwee hardware (no infra to manage)
* SageMaker Experiments. Organize, capture, compare and search your ML jobs
* SageMaker Debugger
  * Saves internal model state at periodical intervals
  * Gradients / tensors over time as a model is trained
  * Define rules for detecting unwanted conditions while training
  * A debug job is run for each rule you configure
  * Logs & fires a CloudWatch event when the rule is hit
* SageMaker Autopilot
  * Automates: 
    * Algorithm selection 
    * Data preprocessing 
    * Model tuning 
    * All infrastructure
  * It does all the trial & error for you 
  * More broadly this is called AutoML
* SageMaker Model Monitor
  * Get alerts on quality deviations on your deployed models
  * Visualize data drift
  * Example: loan model starts giving people more credit due to drifting or missing input features
  * No code needed

## Amazon Comprehend
* Input social media, emails, web pages, documents, transcripts, medical records (Comprehend Medical)
* Extract key phrases, entities, sentiment, language, syntax, topics, and document classifications
* Can train on your own data

## Amazon Translate
* Uses deep learning for translation
* Supports custom terminology 
* In CSV or TMX format
* Appropriate for proper names, brand names, etc.

## Amazon Transcribe
* Speech to text 
  * Input in FLAC, MP3, MP4, or WAV, in a specified language 
  * Streaming audio supported (HTTP/2 or WebSocket) 
    * French, English, Spanish only 
* Speaker Identificiation 
  * Specify number of speakers
* Channel Identification 
  * i.e., two callers could be transcribed separately 
  * Merging based on timing of “utterances”
* Custom Vocabularies
  * Vocabulary Lists (just a list of special words — names, acronyms)
  * vocabulary Tables (can include “SoundsLike”, “IPA”, and “DisplayAs")


## Amazon Polly
* Neural Text-To-Speech, many voices & languages
* Lexicons 
  * Customize pronunciation of specific words & phrases 
  * Example: “World Wide Web Consortium” instead of "w3C

* SSML 
  * Alternative to plain text 
  * Speech Synthesis Markup Language 
  * Gives control over emphasis, pronunciation, breathing, whispering, speech rate, pitch, pauses. 
  * Speech Marks
* Can encode when sentence / word starts and ends in the audio stream
* Useful for lip-synching animation

## Rekognition
* Computer vision
* Object and scene detection 
  * Can use your own face collection
* Image moderation
* Facial analysis
* Celebrity recognition 
* Face comparison
* Text in image
* Video analysis
  * Objects / people / celebrities marked on timeline 
  * People Pathing
* Images come from S3, or provide image bytes as part of request 
  * S3 will be faster if the image is already there 
* Facial recognition depends on good lighting, angle, visibility of eyes, resolution 
* Video must come from Kinesis Video Streams 
  * H.264 encoded 
  * 5-30 FPS 
  * Favor resolution over framerate
* Can use with Lambda to trigger image analysis upon upload
* Train with a small set of labeled images
* Use your own labels for unique items
* Example: the NFL (National Football League in the US) uses custom labels to identify team logos, pylons, and foam fingers in images.

## Amazon Forecast
* Fully-managed service to deliver highly accurate forecasts with ML 
* “AutoML” chooses best model for your time series data 
  * ARIMA, DeepAR, ETS, NPTS, Prophet
* Works with any time series 
  * Price, promotions, economic performance, etc.
  * Can combine with associated data to find relationships
* Inventory planning, financial planning, resource planning
* Based on “dataset groups,” “predictors,” and “forecasts.

## Amazon Lex
* Billed as the inner workings of Alexa 
* Natural-language chatbot engine
* A Bot is built around Intents 
  * Utterances invoke intents (“I want to order a pizza”) 
  * Lambda functions are invoked to fulfill the intent
  * Slots specify extra information needed by the intent
    * Pizza size, toppings, crust type, when to deliver, etc.
* Can deploy to AWS Mobile SDK, Facebook Messenger, Slack, and Twilio

## Other Services
* Amazon Personalize 
  * Recommender system
* Amazon Textract 
  * OCR with forms, fields, tables support
* AWS DeepRacer (Educational)
  * Reinforcement learning powered 1/18- scale race car 
* DeepLens (Educational)
  * Deep learning-enabled video camera
* Integrated with Rekognition, SageMaker, Polly, Tensorflow, MXNet, Caffe

## New ML Services For 2020 and beyond
* AWS DeepComposer
  * Al-powered keyboard
  * Composes a melody into an entire song
  * For educational purposes
* Amazon Fraud Detector
  * High level service for deetcting fraud given your own historical fraud dataset
  * Upload your own historical fraud data
  * Builds custom models from a template you choose
  * Exposes an API for your online application
  * Assess risk from: 
    * New accounts 
    * Guest checkout 
    * “Try before you buy” abuse 
    * Online payments
* Amazon CodeGuru
  * Automated code reviews!
  * Finds lines of code that hurt performance
  * Resource leaks, race conditions
  * Offers specific recommendation
  * Powered by ML
  * Currently Java-only (more coming soon)
* Contact Lens for Amazon Connect
  * For customer support call centers
  * Ingests audio data from recorded calls * Allows search on calls / chats
  * Sentiment analysis
  * Find “utterances” that correlate with successful calls
  * Categorize calls automatically * Measure talk speed and interruptions
  * Theme detection: discovers emerging issues
* Amazon Kendra
  * Enterprise search with natural language
  * For example, “Where is the IT support desk?” “How do | connect to my VPN?"
  * Combines data from file systems, SharePoint, intranet, sharing services. (JDBC, S3) into one searchable repository
  * ML-powered (of course) — uses thumbs up / down feedback
  * Relevance tuning — boost strength of document freshness, view counts, etc.
  * Alexa’s sister? I don’t know, but that’s one way to remember it
* Amazon Augmented Al (A21)
  * Human review of ML predictions
  * Builds workflows for reviewing low-confidence predictions * Access the Mechanical Turk workforce or vendors
  * Integrated into Amazon Textract and Rekognition
  * Integrates with SageMaker
  * Very similar to Ground Truth

## Putting Them Together
* Build your own Alexa!
  * Transcribe -> Lex -> Polly 
* Make a universal translator!
  * Transcribe -> Translate -> Polly 
* Build a Jeff Bezos detector!
  * DeepLens -> Rekognition
* Are people on the phone happy?
  * Transcribe -> Comprehend

## Extra
* Higher values for batch might make training faster but might give a not so optimal solution since it gets stuck easier as it has more samples
* High learning rate might overshoot. Small batch size + small learning rate is the way to go



## Re-check
* Understand PCA
* Understand kmeans++