# 13 - S3

Amazon S3 Overview - Buckets

* Amazon S3 allows people to store objects (files) in “buckets” (directories 
* **Buckets must have a globally unique name **
* **Buckets are defined at the region level**
* Naming convention 
* No uppercase 
* No underscore 
* 3-63 characters long 
* Not an IP 
* Must start with lowercase letter or number

Amazon Overview - Objects

* Objects (files) have a Key
* The **key** is the FULL path: 
  * s3://my-bucket/my_file.bxt 
  * s3://my-bucket/my_folder | /another_folder/my_file.txt
* The key is composed of **prefix** + **object name **
  * s3://my-bucket/my_folder | /another_folder/my_file.txt
* There's no concept of “directories” within buckets (although the UI will trick you to think otherwise)
* Just keys with very long names that contain slashes (‘‘/’”’)

* Object values are the content of the body: 
  * **Max Object Size is 5TB (5O00GB) **
  * If uploading more than 5GB, must use ‘multi-part upload” .

* Metadata (list of text key / value pairs — system or user metadata) 
* Tags (Unicode key / value pair — up to |0) — useful for security / lifecycle
* Version ID (if versioning is enabled)

* S3 is not a Global service but a Global console. The buckets must be created with AZs.
* Pre-signed URL is a URL with temporary credentials information to open an object


## 125 - Amazon S3 - Versioning

* You can version your files in Amazon S3
* It is enabled at the bucket level

* Same key overwrite will increment the “version”: |, 2, 3.... 
* It is best practice to version your buckets
* Protect against unintended deletes (ability to restore a version) 
* Easy roll back to previous version
* Notes:
  * Any file that is not versioned prior to enabling versioning will have version “null” 
  * Suspending versioning does not delete the previous versions
* Deleting a versioned file simply marks it as deleted, it doesn't delete the older versions so it allows recovery
* Suspending a bucket from using versioning doesn't delete older file versions, simply block new ones from being versioned

## 126 - S3 Encryption

* There are 4 methods of encrypting objects in $3
  * SSE-S3: encrypts S3 objects using keys handled & managed by AWS
  * SSE-KMS: leverage AWS Key Management Service to manage encryption keys
  * SSE-C: when you want to manage your own encryption keys
  * Client Side Encryption

### SSE-S3
* SSE means Server Side Encryption
* SSE-S3: encryption using keys handled & managed by Amazon S3 
* Object is encrypted server side
* AES-256 encryption type
* Must set header: ‘x-amz-server-side-encryption”: "AES256"
* The data key is managed and owned by S3

### SSE-KMS
* SSE-KMS: encryption using keys handled & managed by KMS 
* KMS Advantages: user control + audit trail Object is encrypted server side
* Must set header: x-amz-server-side-encryption”: "aws:kms"

### SSE-C
* SSE-C: server-side encryption using data keys fully managed by the customer outside of AWS Amazon S3 does not store the encryption key you provide
* HTTPS must be used
* Encryption key must provided in HTTP headers, for every HTTP request made
* Only available through the CLI


### Client Side Encryption (CSE)

* Client library such as the Amazon S3 Encryption Client
* Clients must encrypt data themselves before sending to S3
* Clients must decrypt data themselves when retrieving from $3 
* Customer fully manages the keys and encryption cycle


* Encryption file is specific for file and version ID

## 128 - Security & Bucket Policies

* User based 
  * IAM policies - which API calls should be allowed for a specific user from IAM console
* Resource Based 
  * Bucket Policies - bucket wide rules from the S3 console - allows cross account 
  * Object Access Control List (ACL) — finer grain 
  * Bucket Access Control List (ACL) — less common

* Note: an IAM principal can access an S3 object if:
  * the user [AM permissions allow it OR the resource policy ALLOWS it 
  * AND there's no explicit DENY

### S3 Bucket Policies

* JSON based policies 
  * Resources: buckets and objects 
  * Actions: Set of API to Allow or Deny 
  * Effect: Allow / Deny
  * Principal: The account or user to apply the policy to

* Use S3 bucket for policy to: 
  * Grant public access to the bucket 
  * Force objects to be encrypted at upload
  * Grant access to another account (Cross Account)

There's an account level control of public access.

* Networking:
  * Supports VPC Endpoints (for instances in VPC without www internet) 
* Logging and Audit:
  * S3 Access Logs can be stored in other S3 bucket
  * API calls can be logged in AWS CloudTrail
* User Security: 
  * MFA Delete: MFA (multi factor authentication) can be required in versioned buckets to delete objects 
  * Pre-Signed URLs: URLs that are valid only for a limited time (ex: premium video service for logged in users)

## 131 - CORS

* CORS means Cross-Origin Resource Sharing
* Web Browser based mechanism to allow requests to other origins while visiting the main origin
* Same origin: http://example.com/app| & http://example.com/app2
* Different origins: http:/Awww.example.com & http://otherexample.com
* The requests won't be fulfilled unless the other origin allows for the requests, using CORS Headers (ex: Access-Control-Allow-Origin)
* If a client does a cross-origin request on our $3 bucket, we need to enable the correct CORS headers
* It's a popular exam question 
* You can allow for a specific origin or for * (all origins)
* The CORS is specified in the destination bucket (the one with the content being called)

## 133 -Amazon S3 - Consistency Model

* Strong consistency as of December 2020: 
* After a:
  * successful write of a new object (new PUT)
  * or an overwrite or delete of an existing object (overwrite PUT or DELETE) 
* ...any:
  * subsequent read request immediately receives the latest version of the object (read after write consistency)
  * subsequent list request immediately reflects changes (list consistency)
* Available at no additional cost, without any performance impact